In this section, we take a close look at the proposed structure and capabilities of the framework's API.
The final implementation will however deviate with a high probability.

\subsection{Proposed Functionality}

The framework shall have two primary capabilities: first, it should enable fast and easy access to the 3d space calculated from the marker.
Second, given a model to display, it should be capable of returning a rendering of the object within the scene.

Generally, the framework shall work like in the following.
Upon import of the framework into the project, the program passes the image to the framework.
This shall be possible to do in two ways: one, let OpenCV read directly from the camera, or two, give the framework the frames from the program manually.
This allows the possibility of using other video or picture streams with the framework\footnote{For example, rendering an object into a video.}.

Now the magic happens; OpenCV calculates the 3d space.
Before passing it on to the framework, the program can read the 3d space.
This direct access to the resulting coordinate space allows a more down-to-earth programming should it be required.
If the program is only interested in the final rendered frame, given a model, the framework then renders the object into the scene and returns the frame to the program.

Apart from the above mentioned basic functionality capabilities, the framework should also handle the easy gathering of performance and error information.
Accessing the information will be done via a listener or an equal mechanism so that any developers can choose when and how to use the information collected by the framework.
Care should be taken to ensure that the error collection can run even when the framework encounters problems, possibly by putting it in a separat thread.

\begin{figure}
	\centering
	\includegraphics[width=12cm]{images/sequence_access.eps}
	\caption[Access Sequence.]{The proposed sequence of the workflow with the framework, using OpenCV.}
	\label{fig:sequence_access}
\end{figure}

Figure \ref{fig:sequence_access} shows the proposed outside view of the framework, and where data can be input or read.
As proposed, the framework should offer a wide variety of uses, without being overly complex.
Another important aspect we want to make possible is the possibility of changing all the more important parameters during runtime, such as switching the model or adding a new marker.
This should allow a much smoother usage of the framework and any derived apps as a result.

\subsection{Limitations of Scope}

TODO: Text (nice-to-have)

\begin{tabulary}{\textwidth}{L || L}
Animated Objects & Allow the object to have an animation and offer control access to it.\\
\end{tabulary}

The following are features that will not be implemented.
However, where possible, the framework will allow easy adaption to extend its functionality beyond what it will offer from the start.

\begin{tabulary}{\textwidth}{L || L}
Marker-less Tracking & Marker-less tracking will most likely not be within the scope of the initial framework. However, as changing markers during runtime will probably be possible, adding this feature should not be overly difficult.\\
\hline
Occlusion & The capability to detect where scene occlusion is taking place is definitely beyond the scope of the framework, as this requires extensively more work.\\
\hline
Fancy Rendering & For now, no support of stereoscopic rendering of any kind. Depending on the difficulty of implementation, support for different direct rendering engines within the framework might be included.\\
\end{tabulary}

\subsection{Class Diagram}

\begin{figure}
	\centering
	\includegraphics[width=10cm]{images/class_diagram.eps}
	\caption[General Class Diagram]{This is the preliminary class diagram of the proposed functionality of the framework. Note the low number of public methods: this should make usage of the framework very easy. Also note that the Messenger class has no dependencies towards other classes to enable it to run as a distraction-free subthread from the rest of the framework.}
	\label{fig:class_diagram}
\end{figure}

Now let us take a look at the proposed class structure within the framework, as seen in Figure \ref{fig:class_diagram}.
Main access is controlled over the framework controller, which almost exclusively contains all the public methods callable from outside of the framework.
Only the Messanger class offers public access too.

The Messanger class allows easy and quick access to any debugging, logging, or error messages to outside classes via a listener principle.
Allowing outside threads to be directly notified when a message is written enables comparatively fail-safe information gathering.
This should also help with the transparency of using the framework.
Note that to keep the Messanger thread-safe, the register and remove methods for listeners are one atomic protected block.
The Messanger thread itself is controlled from the framework with default behaviour enabled should it stop running.

The Controller encapsulates all the primmary functionality within the framework.
Apart from managing all the other classes on startup and closure, it also handles the information exchange between them and the caller.
It also enables the management of the trackables and their assigned renderables.
Changing the source for the video to detect markers and changing the rendering engine can be done here too, if desired.
The Controller also handles any inter-system compatibility that the passage of information between the Renderer and Wrapper require.

The OpenGL ES Renderer class handles the generation of the images for the output in the form of a canvas.
It takes the 3d coordinate system from the OpenCV wrapper and renders the objects with the given specifications onto the final canvas.

Last is the OpenCV Wrapper. The Wrapper handles the control of the OpenCV library and does any managing work requried for the framework.
It also enables some source and destination control for respectively the images and 3d coordinate space.

The above proposed system should enable an easily extensible build for the framework.
For example, using the framework with any OpenCV version apart from the Android port should be achievable by simply writting an extra wrapper.
The same applies for the Renderer; it can simply be replaced by new modules that render to DirectX or the full OpenGL specification.
While we encourage such modularity, we will not promise it for the final version of this work.

\subsection{Application Programming Interface}

The following represents the suggested interface for the framework.
With the listed methods, all functionality that the framework offers can be accessed.

TODO: Place javadoc here.

\subsection{Usage}

\begin{figure}
	\centering
	\includegraphics[width=4cm]{images/marker_example.png}
	\caption[Example Marker.]{An example of a marker. This image is either printed or displayed by some other means in the real world to allow a system to use it as a reference to base a virtual overlay off of it.}
	\label{fig:marker_example}
\end{figure}

To enable the framework to detect a 3d coordinate system from a video feed, a marker will be required.
A marker is a visually significant pattern that the system can detect within an image and be used to calculate spatial coordinates. Figure \ref{fig:marker_example} shows an example for such a marker.

Depending on the capabilities of the finished framework, it could be possible to track multiple markers in a single instance.
This would allow multiple objects to be rendered simultaneously, increasing the use cases for the framework.

To enable the functionality, the recorded or live video stream must have a pre-defined marker somewhere in it.
This can be a screen showing the marker, a printed marker, or any other way of displaying a marker in a scene.
Depending on the capabilities of OpenCV, the framework will then detect its relative 3d position.
With the detected space, the framework can now render objects into the scene with the correct rotation, scaling, and perspective.

The framework should be made to be thread safe.
Internally the framework will most likely at least have one sub thread for error and information collection and propagation.
If possible, it should be easy to put the framework into it's own thread domain so that any programs using it do not have to worry about locking down their threads by accessing the framework.
This means that the framework will most likely work asynchronous, with the possibility of forcing it to run synchronous where required.

